#!/usr/bin/env python3
"""
Python script generated from: Week6/è«–æ–‡é–±è®€åŠ©æ‰‹.md
Generated on: 1760867432.7444682
Note: Colab-specific commands (!pip, %magic) have been commented out
"""

# COLAB ONLY: !pip install openai gradio pypdf2

from google.colab import userdata
import os

api_key = userdata.get('OpenAI')
os.environ['OPENAI_API_KEY'] = api_key

from openai import OpenAI
import gradio as gr
import PyPDF2
from dataclasses import dataclass
from typing import List, Dict, Optional, Any

client = OpenAI()
MODEL_NAME = "gpt-5"

def extract_pdf_text(pdf_path: str) -> str:
    """
    å¾ä¸Šå‚³çš„ PDF æª”æ¡ˆä¸­æå–æ–‡å­—å…§å®¹

    é€™å€‹å‡½æ•¸æœƒï¼š
    1. é€é è®€å– PDF å…§å®¹
    2. éæ¿¾æ‰ç©ºç™½é é¢
    3. æ¨™è¨˜é ç¢¼æ–¹ä¾¿å®šä½
    4. é™åˆ¶æœ€å¤§é•·åº¦é¿å…è¶…é Token é™åˆ¶

    Args:
        pdf_path: PDF æª”æ¡ˆè·¯å¾‘

    Returns:
        str: æå–çš„æ–‡å­—å…§å®¹

    Raises:
        ValueError: ç•¶ PDF ç„¡æ³•è®€å–æˆ–å…§å®¹ç‚ºç©ºæ™‚
    """
    if not pdf_path:
        raise ValueError("æœªæä¾› PDF æª”æ¡ˆ")

    text_segments: List[str] = []

    try:
        with open(pdf_path, "rb") as handle:
            pdf_reader = PyPDF2.PdfReader(handle)

            if not pdf_reader.pages:
                raise ValueError("PDF ä¸­æ²’æœ‰å¯ç”¨é é¢")

            # é€é æå–æ–‡å­—
            for index, page in enumerate(pdf_reader.pages, start=1):
                # æ³¨æ„ï¼špage.extract_text() å¯èƒ½å›å‚³ Noneï¼Œéœ€è¦è™•ç†
                page_text = page.extract_text() or ""

                # åªåŠ å…¥æœ‰å…§å®¹çš„é é¢
                if page_text.strip():
                    text_segments.append(f"\n--- Page {index} ---\n{page_text.strip()}")

    except Exception as exc:
        raise ValueError(f"PDF è®€å–å¤±æ•—: {exc}") from exc

    if not text_segments:
        raise ValueError("PDF ä¸­æ²’æœ‰å¯è®€å–çš„æ–‡å­—å…§å®¹")

    # åˆä½µæ‰€æœ‰é é¢
    combined = "".join(text_segments)

    # é˜²æ­¢è¶…é Token é™åˆ¶ï¼ˆç´„ 15000 å­—å…ƒï¼‰
    MAX_PDF_CHARS = 15000
    if len(combined) > MAX_PDF_CHARS:
        combined = combined[:MAX_PDF_CHARS] + "\n\n... (å…§å®¹éé•·ï¼Œå·²æˆªæ–·ã€‚è«‹åˆ†æ®µæå•ä»¥ç²å¾—å®Œæ•´è§£èªªã€‚)"

    return combined

SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è«–æ–‡é–±è®€åŠ©æ‰‹ï¼Œå°ˆé–€å¹«åŠ©å­¸ç”Ÿç†è§£å­¸è¡“è«–æ–‡ã€‚

**ä½ çš„æ•™å­¸åŸå‰‡**ï¼š

1. **è²»æ›¼å­¸ç¿’æ³• (Feynman Technique)**ï¼š
   - ç”¨æœ€ç°¡å–®çš„èªè¨€è§£é‡‹è¤‡é›œæ¦‚å¿µ
   - ä½¿ç”¨é¡æ¯”å’Œæ—¥å¸¸ç”Ÿæ´»çš„ä¾‹å­
   - é¿å…éåº¦ä½¿ç”¨å°ˆæ¥­è¡“èªï¼Œå¿…è¦æ™‚è¦å…ˆè§£é‡‹

2. **è˜‡æ ¼æ‹‰åº•å¼æå• (Socratic Method)**ï¼š
   - ä¸ç›´æ¥çµ¦ç­”æ¡ˆï¼Œè€Œæ˜¯å¼•å°å­¸ç”Ÿæ€è€ƒ
   - æå‡ºå•Ÿç™¼æ€§å•é¡Œï¼Œå¹«åŠ©å­¸ç”Ÿè‡ªå·±æ‰¾åˆ°ç­”æ¡ˆ
   - é¼“å‹µæ‰¹åˆ¤æ€§æ€ç¶­

3. **çµæ§‹åŒ–åˆ†æ**ï¼š
   - å¹«åŠ©å­¸ç”Ÿç†è§£è«–æ–‡çµæ§‹ï¼šæ‘˜è¦ã€å¼•è¨€ã€æ–¹æ³•ã€çµæœã€çµè«–
   - æŒ‡å‡ºè«–æ–‡çš„æ ¸å¿ƒè²¢ç»å’Œå‰µæ–°é»
   - è§£é‡‹ç ”ç©¶æ–¹æ³•å’Œå¯¦é©—è¨­è¨ˆ

4. **å‹å–„äº’å‹•**ï¼š
   - ä»¥é¼“å‹µå’Œæ”¯æŒçš„èªæ°£å›æ‡‰
   - ç¢ºèªå­¸ç”Ÿç†è§£å¾Œå†ç¹¼çºŒ
   - å¯ä»¥ç”¨ emoji è®“å°è©±æ›´ç”Ÿå‹•

**ç•¶å­¸ç”Ÿä¸Šå‚³è«–æ–‡å¾Œ**ï¼š
- ç­‰å¾…å­¸ç”Ÿæå•ï¼Œä¸è¦ä¸»å‹•æ‘˜è¦
- æ ¹æ“šå­¸ç”Ÿçš„å•é¡Œï¼Œå¾è«–æ–‡ä¸­æ‰¾åˆ°ç›¸é—œå…§å®¹å›ç­”
- ç¢ºä¿å›ç­”æº–ç¢ºä¸”åŸºæ–¼è«–æ–‡å…§å®¹

**è¨˜ä½**ï¼šä½ çš„ç›®æ¨™æ˜¯å¹«åŠ©å­¸ç”Ÿã€Œå­¸æœƒå¦‚ä½•è®€è«–æ–‡ã€ï¼Œè€Œä¸åªæ˜¯ã€Œè®€æ‡‚é€™ç¯‡è«–æ–‡ã€ã€‚"""

WELCOME_MESSAGE = """ğŸ‘‹ å—¨ï¼æˆ‘æ˜¯ä½ çš„**è«–æ–‡é–±è®€åŠ©æ‰‹ & AI å°è©±å¤¥ä¼´**ï¼

ğŸ“š **æˆ‘èƒ½å¹«ä½ åšä»€éº¼ï¼Ÿ**

**æ¨¡å¼ 1ï¼šè«–æ–‡é–±è®€åŠ©æ‰‹** ğŸ“„
- ä¸Šå‚³ PDF è«–æ–‡å¾Œï¼Œæˆ‘æœƒå¹«ä½ ï¼š
  - ç”¨ç°¡å–®çš„èªè¨€è§£é‡‹è«–æ–‡ä¸­çš„è¤‡é›œæ¦‚å¿µ
  - å¹«ä½ ç†è§£ç ”ç©¶æ–¹æ³•å’Œå¯¦é©—è¨­è¨ˆ
  - å¼•å°ä½ æ€è€ƒè«–æ–‡çš„æ ¸å¿ƒè²¢ç»
  - å›ç­”ä½ å°è«–æ–‡å…§å®¹çš„ä»»ä½•ç–‘å•

**æ¨¡å¼ 2ï¼šä¸€èˆ¬ AI åŠ©æ‰‹** ğŸ’¬
- ä¸ä¸Šå‚³ PDF ä¹Ÿå¯ä»¥ç›´æ¥è·Ÿæˆ‘èŠå¤©ï¼š
  - å­¸ç¿’ä»»ä½•ä¸»é¡Œ
  - è§£ç­”å•é¡Œ
  - è¨è«–æƒ³æ³•
  - å¯«ä½œå”åŠ©

ğŸš€ **å¦‚ä½•é–‹å§‹ï¼Ÿ**
- **æƒ³è®€è«–æ–‡ï¼Ÿ** é»æ“Šä¸Šæ–¹ã€Œä¸Šå‚³ PDFã€æŒ‰éˆ•
- **æƒ³èŠå¤©ï¼Ÿ** ç›´æ¥é–‹å§‹æå•å³å¯ï¼

ğŸ’¡ **æå•ç¯„ä¾‹**ï¼š
- ğŸ“– è«–æ–‡ç›¸é—œï¼šã€Œé€™ç¯‡è«–æ–‡çš„ä¸»è¦è²¢ç»æ˜¯ä»€éº¼ï¼Ÿã€
- ğŸ’¬ ä¸€èˆ¬å°è©±ï¼šã€Œè§£é‡‹ä¸€ä¸‹æ©Ÿå™¨å­¸ç¿’çš„åŸºæœ¬æ¦‚å¿µã€

æº–å‚™å¥½äº†å—ï¼Ÿé–‹å§‹ä½ çš„æ¢ç´¢ä¹‹æ—…å§ï¼ ğŸš€âœ¨"""

PDF_CONTEXT_TEMPLATE = (
    "ä»¥ä¸‹æ˜¯ä½¿ç”¨è€…æä¾›çš„è«–æ–‡å…§å®¹ (æª”å: {filename}, ç‰ˆæœ¬: {version})ï¼Œå›ç­”æ™‚å‹™å¿…å¼•ç”¨æ­¤å…§å®¹ï¼š\n"
    "{content}"
)

@dataclass
class PDFState:
    """
    ç®¡ç† PDF ç‹€æ…‹çš„è³‡æ–™é¡åˆ¥

    Attributes:
        filename: PDF æª”å
        content: æå–çš„æ–‡å­—å…§å®¹
        version: PDF ç‰ˆæœ¬è™Ÿï¼ˆæ¯æ¬¡ä¸Šå‚³æ–° PDF æœƒéå¢ï¼‰
    """
    filename: Optional[str] = None
    content: Optional[str] = None
    version: int = 0

    def context_message(self) -> Optional[Dict[str, str]]:
        """
        ç”¢ç”ŸåŒ…å« PDF å…§å®¹çš„è¨Šæ¯ç‰©ä»¶

        é€™å€‹è¨Šæ¯æœƒåœ¨æ¯æ¬¡ API å‘¼å«æ™‚æ’å…¥ï¼Œç¢ºä¿æ¨¡å‹çŸ¥é“ç•¶å‰çš„ PDF å…§å®¹ã€‚
        ä½¿ç”¨ç‰ˆæœ¬è™Ÿå¯ä»¥è®“æ¨¡å‹å€åˆ†ä¸åŒçš„ PDFã€‚

        Returns:
            åŒ…å« PDF å…§å®¹çš„ user è¨Šæ¯ï¼Œå¦‚æœæ²’æœ‰ PDF å‰‡å›å‚³ None
        """
        if not self.content or not self.filename:
            return None

        return {
            "role": "user",
            "content": PDF_CONTEXT_TEMPLATE.format(
                filename=self.filename,
                version=self.version,
                content=self.content,
            ),
        }


# å…¨åŸŸç‹€æ…‹è®Šæ•¸
conversation_history: List[Dict[str, str]] = []  # å„²å­˜å°è©±æ­·å²ï¼ˆuser å’Œ assistant è¨Šæ¯ï¼‰
last_response_id: Optional[str] = None  # Response API çš„ previous_response_id
pdf_state = PDFState()  # PDF ç‹€æ…‹

def summarise_outputs(response: Any) -> str:
    """
    å¾ Response API çš„å›æ‡‰ä¸­æå–æ–‡å­—å…§å®¹

    Response API çš„è¼¸å‡ºæ ¼å¼æ¯”è¼ƒè¤‡é›œï¼Œå¯èƒ½åŒ…å«ï¼š
    - output_text: ç›´æ¥çš„æ–‡å­—è¼¸å‡ºï¼ˆæœ€å¸¸è¦‹ï¼‰
    - output: åŒ…å«å¤šå€‹ message ç‰©ä»¶çš„é™£åˆ—ï¼ˆéœ€è¦æ‰‹å‹•è§£æï¼‰

    é€™å€‹å‡½æ•¸æœƒå˜—è©¦å…©ç¨®æ–¹å¼ï¼Œç¢ºä¿èƒ½å–å¾—å…§å®¹ã€‚

    Args:
        response: OpenAI Response API çš„å›æ‡‰ç‰©ä»¶

    Returns:
        str: æå–çš„æ–‡å­—å…§å®¹
    """
    # å„ªå…ˆä½¿ç”¨ output_text
    if getattr(response, "output_text", None):
        return response.output_text

    # å¦‚æœæ²’æœ‰ output_textï¼Œæ‰‹å‹•è§£æ output é™£åˆ—
    collected: List[str] = []
    for item in getattr(response, "output", []) or []:
        if item.get("type") != "message":
            continue
        for chunk in item.get("content", []):
            if chunk.get("type") == "output_text" and chunk.get("text"):
                collected.append(chunk["text"])

    return "".join(collected).strip()


def ensure_history(history: Optional[List[List[str]]]) -> List[List[str]]:
    """
    ç¢ºä¿ Gradio history æ˜¯æœ‰æ•ˆçš„åˆ—è¡¨

    Gradio åœ¨ç¬¬ä¸€æ¬¡å‘¼å«æ™‚å¯èƒ½å‚³å…¥ Noneï¼Œé€™æœƒå°è‡´éŒ¯èª¤ã€‚
    é€™å€‹å‡½æ•¸ç¢ºä¿æˆ‘å€‘ç¸½æ˜¯æœ‰ä¸€å€‹æœ‰æ•ˆçš„åˆ—è¡¨å¯ä»¥æ“ä½œã€‚

    Args:
        history: Gradio å‚³å…¥çš„å°è©±æ­·å²ï¼ˆå¯èƒ½æ˜¯ Noneï¼‰

    Returns:
        æœ‰æ•ˆçš„å°è©±æ­·å²åˆ—è¡¨
    """
    return list(history) if history else []

def chat_with_paper(message: str, history: Optional[List[List[str]]]):
    """
    è™•ç†ä½¿ç”¨è€…è¨Šæ¯ä¸¦ç”¢ç”Ÿå›æ‡‰

    **é‡è¦æ”¹é€²**ï¼ˆç›¸è¼ƒæ–¼åŸæœ¬çš„å¯¦ä½œï¼‰ï¼š
    1. âœ… æ­£ç¢ºå„²å­˜ user å’Œ assistant è¨Šæ¯åˆ° conversation_history
    2. âœ… æ¯æ¬¡å‘¼å«éƒ½é‡æ–°æ³¨å…¥ PDF å…§å®¹ï¼ˆæ”¯æ´é‡æ–°ä¸Šå‚³ï¼‰
    3. âœ… ä½¿ç”¨ previous_response_id ç¶­è­· Response API çš„ç‹€æ…‹
    4. âœ… è™•ç† history=None çš„é‚Šç•Œæƒ…æ³
    5. âœ… è™•ç†ç©ºç™½è¼¸å‡ºçš„æƒ…æ³

    æ”¯æ´å…©ç¨®æ¨¡å¼ï¼š
    1. æœ‰ PDFï¼šè«–æ–‡é–±è®€åŠ©æ‰‹æ¨¡å¼
    2. ç„¡ PDFï¼šä¸€èˆ¬ AI åŠ©æ‰‹æ¨¡å¼

    Args:
        message: ä½¿ç”¨è€…ç•¶å‰è¼¸å…¥
        history: Gradio èŠå¤©æ­·å² [[user_msg, bot_msg], ...]

    Returns:
        list: æ›´æ–°å¾Œçš„ Gradio æ­·å²è¨˜éŒ„ï¼ˆå¿…é ˆæ˜¯ list of lists æ ¼å¼ï¼‰
    """
    global conversation_history, last_response_id

    # ç¢ºä¿ history æ˜¯æœ‰æ•ˆçš„åˆ—è¡¨
    history = ensure_history(history)

    # éæ¿¾ç©ºç™½è¨Šæ¯
    user_message = (message or "").strip()
    if not user_message:
        return history

    # === æ­¥é©Ÿ 1: å»ºæ§‹è¨Šæ¯é™£åˆ— ===
    messages: List[Dict[str, str]] = [
        {"role": "developer", "content": SYSTEM_PROMPT}
    ]

    # === æ­¥é©Ÿ 2: å¦‚æœæœ‰ PDFï¼Œæ³¨å…¥ PDF å…§å®¹ ===
    # æ³¨æ„ï¼šæ¯æ¬¡éƒ½é‡æ–°æ³¨å…¥ï¼Œé€™æ¨£é‡æ–°ä¸Šå‚³ PDF æ™‚æ¨¡å‹æœƒçŸ¥é“
    pdf_context = pdf_state.context_message()
    if pdf_context:
        messages.append(pdf_context)

    # === æ­¥é©Ÿ 3: åŠ å…¥å°è©±æ­·å² ===
    # é€™è£¡åŒ…å«ä¹‹å‰æ‰€æœ‰çš„ user å’Œ assistant è¨Šæ¯
    messages.extend(conversation_history)

    # === æ­¥é©Ÿ 4: åŠ å…¥ç•¶å‰ä½¿ç”¨è€…è¨Šæ¯ ===
    messages.append({"role": "user", "content": user_message})

    # === æ­¥é©Ÿ 5: æº–å‚™ API è«‹æ±‚ ===
    request_payload = {
        "model": MODEL_NAME,
        "input": messages,
        "reasoning": {"effort": "medium"},
        "text": {"verbosity": "medium"}
    }

    # å¦‚æœæœ‰ä¸Šä¸€æ¬¡çš„ response_idï¼ŒåŠ å…¥ä»¥ç¶­æŒæ¨ç†é€£çºŒæ€§
    if last_response_id:
        request_payload["previous_response_id"] = last_response_id

    try:
        # === æ­¥é©Ÿ 6: å‘¼å« OpenAI Response API ===
        response = client.responses.create(**request_payload)

        # === æ­¥é©Ÿ 7: æå–å›æ‡‰æ–‡å­— ===
        assistant_reply = summarise_outputs(response)

        # å¦‚æœæ²’æœ‰æ–‡å­—è¼¸å‡ºï¼ˆç½•è¦‹ä½†å¯èƒ½ç™¼ç”Ÿï¼‰ï¼Œæä¾›å‹å–„çš„éŒ¯èª¤è¨Šæ¯
        if not assistant_reply:
            assistant_reply = "âš ï¸ æ¨¡å‹æœªå›å‚³æ–‡å­—ï¼Œå¯å†è©¦ä¸€æ¬¡æˆ–èª¿æ•´å•é¡Œã€‚"

        # === æ­¥é©Ÿ 8: æ›´æ–°å°è©±æ­·å²ï¼ˆé‡è¦ï¼ï¼‰===
        # å„²å­˜ user å’Œ assistant è¨Šæ¯ï¼Œé€™æ¨£ä¸‹æ¬¡å‘¼å«æ™‚æ¨¡å‹æ‰çŸ¥é“ä¹‹å‰çš„å°è©±
        conversation_history.append({"role": "user", "content": user_message})
        conversation_history.append({"role": "assistant", "content": assistant_reply})

        # === æ­¥é©Ÿ 9: å„²å­˜ response_id ===
        last_response_id = getattr(response, "id", None)

        # === æ­¥é©Ÿ 10: æ›´æ–° Gradio é¡¯ç¤ºçš„æ­·å² ===
        history.append([user_message, assistant_reply])
        return history

    except Exception as exc:
        # éŒ¯èª¤è™•ç†ï¼šåŒæ¨£å›å‚³ Gradio æ ¼å¼
        error_message = f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{exc}\n\nè«‹æª¢æŸ¥ç¶²è·¯é€£ç·šèˆ‡ API è¨­å®šå¾Œå†è©¦ä¸€æ¬¡ã€‚"
        history.append([user_message, error_message])
        return history


def upload_pdf(pdf_file: Optional[str]):
    """
    è™•ç† PDF ä¸Šå‚³

    **é‡è¦æ”¹é€²**ï¼š
    1. âœ… æ›´æ–° pdf_state çš„ç‰ˆæœ¬è™Ÿï¼Œè®“æ¨¡å‹çŸ¥é“æ˜¯æ–°çš„ PDF
    2. âœ… ä¿ç•™ conversation_historyï¼ˆå°è©±æ­·å²ä¸æœƒå› ç‚ºä¸Šå‚³ PDF è€Œæ¶ˆå¤±ï¼‰
    3. âœ… ä¸‹æ¬¡æå•æ™‚æœƒè‡ªå‹•æ³¨å…¥æ–°çš„ PDF å…§å®¹

    Args:
        pdf_file: Gradio ä¸Šå‚³çš„æª”æ¡ˆè·¯å¾‘

    Returns:
        str: ä¸Šå‚³ç‹€æ…‹è¨Šæ¯
    """
    global pdf_state

    if pdf_file is None:
        return "âŒ è«‹é¸æ“‡ PDF æª”æ¡ˆ"

    try:
        # æå– PDF æ–‡å­—
        content = extract_pdf_text(pdf_file)
    except ValueError as exc:
        # å¦‚æœæå–å¤±æ•—ï¼Œé‡ç½® PDF ç‹€æ…‹
        pdf_state = PDFState()
        return f"âŒ {exc}"

    # æ›´æ–° PDF ç‹€æ…‹ï¼ˆç‰ˆæœ¬è™Ÿéå¢ï¼‰
    pdf_state = PDFState(
        filename=os.path.basename(pdf_file),
        content=content,
        version=pdf_state.version + 1,
    )

    # è¨ˆç®—çµ±è¨ˆè³‡è¨Š
    page_count = content.count("--- Page") or "?"
    char_count = len(content)

    # ç”¢ç”Ÿå‹å–„çš„æˆåŠŸè¨Šæ¯
    note = (
        "âœ… PDF ä¸Šå‚³æˆåŠŸï¼\n\n"
        f"ğŸ“„ æª”åï¼š{pdf_state.filename}\n"
        f"ğŸ“„ ç‰ˆæœ¬ï¼š{pdf_state.version}\n"
        f"ğŸ“„ é é¢æ•¸ï¼šç´„ {page_count}\n"
        f"ğŸ”¤ æ–‡å­—é•·åº¦ï¼šç´„ {char_count:,} å­—å…ƒ\n\n"
        "ğŸ’¬ ä½ å¯ä»¥ç›´æ¥æå•ï¼Œæˆ‘æœƒä¾æ“šæœ€æ–°çš„ PDF å›ç­”ã€‚"
    )

    return note


def clear_conversation():
    """
    æ¸…é™¤å°è©±æ­·å²ï¼Œé‡æ–°é–‹å§‹

    æ³¨æ„ï¼šåªæ¸…é™¤å°è©±æ­·å²ï¼ŒPDF è¨­å®šä¿æŒä¸è®Š

    Returns:
        tuple: (æ¸…ç©ºçš„èŠå¤©æ­·å², ç‹€æ…‹è¨Šæ¯)
    """
    global conversation_history, last_response_id

    conversation_history = []
    last_response_id = None

    return [], "ğŸ”„ å°è©±å·²æ¸…é™¤ï¼PDF è¨­å®šä¿æŒä¸è®Šã€‚"

# å»ºç«‹ Gradio ä»‹é¢
with gr.Blocks(title="è«–æ–‡é–±è®€åŠ©æ‰‹", theme=gr.themes.Soft()) as demo:

    gr.Markdown("# ğŸ“š è«–æ–‡é–±è®€åŠ©æ‰‹ - Paper Reading Assistant")
    gr.Markdown("åŸºæ–¼ OpenAI GPT-5 Response APIï¼Œçµåˆè²»æ›¼å­¸ç¿’æ³•èˆ‡è˜‡æ ¼æ‹‰åº•å¼æå•")

    with gr.Row():
        with gr.Column(scale=3):
            # PDF ä¸Šå‚³å€
            pdf_upload = gr.File(
                label="ğŸ“„ ä¸Šå‚³è«–æ–‡ PDF",
                file_types=[".pdf"],
                type="filepath"
            )
            upload_status = gr.Textbox(
                label="ä¸Šå‚³ç‹€æ…‹",
                value=WELCOME_MESSAGE,
                interactive=False,
                lines=10
            )

        with gr.Column(scale=7):
            # èŠå¤©å€
            chatbot = gr.Chatbot(
                label="ğŸ’¬ å°è©±å€",
                height=500,
                show_label=True,
                type="tuples"  # æ˜ç¢ºæŒ‡å®šä½¿ç”¨ tuples æ ¼å¼
            )

            msg_input = gr.Textbox(
                label="è¼¸å…¥ä½ çš„å•é¡Œ",
                placeholder="ä¾‹å¦‚ï¼šé€™ç¯‡è«–æ–‡çš„ä¸»è¦è²¢ç»æ˜¯ä»€éº¼ï¼Ÿ",
                lines=2
            )

            with gr.Row():
                submit_btn = gr.Button("ğŸ“¤ é€å‡º", variant="primary")
                clear_btn = gr.Button("ğŸ”„ æ¸…é™¤å°è©±")

    # äº‹ä»¶ç¶å®š
    pdf_upload.change(
        fn=upload_pdf,
        inputs=pdf_upload,
        outputs=upload_status
    )

    submit_btn.click(
        fn=chat_with_paper,
        inputs=[msg_input, chatbot],
        outputs=chatbot
    ).then(
        lambda: "",  # æ¸…ç©ºè¼¸å…¥æ¡†
        outputs=msg_input
    )

    msg_input.submit(
        fn=chat_with_paper,
        inputs=[msg_input, chatbot],
        outputs=chatbot
    ).then(
        lambda: "",  # æ¸…ç©ºè¼¸å…¥æ¡†
        outputs=msg_input
    )

    clear_btn.click(
        fn=clear_conversation,
        outputs=[chatbot, upload_status]
    )

    # èªªæ˜å€
    gr.Markdown("""
    ---
    ### ğŸ’¡ ä½¿ç”¨æŠ€å·§

    - **ç¬¬ä¸€æ¬¡æå•**ï¼šå»ºè­°å…ˆå•ã€Œé€™ç¯‡è«–æ–‡åœ¨ç ”ç©¶ä»€éº¼ï¼Ÿã€äº†è§£å…¨è²Œ
    - **æ·±å…¥ç†è§£**ï¼šé‡å°ä¸æ‡‚çš„ç« ç¯€æˆ–æ¦‚å¿µæå•
    - **æ‰¹åˆ¤æ€è€ƒ**ï¼šå¯ä»¥å•ã€Œé€™å€‹æ–¹æ³•æœ‰ä»€éº¼é™åˆ¶ï¼Ÿã€
    - **æ¸…é™¤å°è©±**ï¼šæƒ³é‡æ–°é–‹å§‹æ™‚ï¼Œé»æ“Šã€Œæ¸…é™¤å°è©±ã€æŒ‰éˆ•
    - **é‡æ–°ä¸Šå‚³ PDF**ï¼šå¯ä»¥éš¨æ™‚ä¸Šå‚³æ–°çš„ PDFï¼Œå°è©±æ­·å²æœƒä¿ç•™

    ### âš™ï¸ æŠ€è¡“èªªæ˜

    - **æ¨¡å‹**ï¼šOpenAI GPT-5 (Response API)
    - **æ¨ç†ç­‰ç´š**ï¼šMedium (å¹³è¡¡é€Ÿåº¦èˆ‡å“è³ª)
    - **PDF è™•ç†**ï¼šPyPDF2 (å®Œæ•´æ–‡å­—æå–)
    - **ä»‹é¢æ¡†æ¶**ï¼šGradio 5.x

    ---
    *Made with â¤ï¸ for NCCU AI Course*
    """)

# å•Ÿå‹• Gradio æ‡‰ç”¨
demo.launch(share=True, debug=True)

# âŒ èˆŠç‰ˆ Chat Completions API
response = client.chat.completions.create(
    model="gpt-4",
    messages=[...]
)
reply = response.choices[0].message.content

# âœ… æ–°ç‰ˆ Response API
response = client.responses.create(
    model="gpt-5",
    input=[...],
    reasoning={"effort": "medium"},
    text={"verbosity": "medium"}
)
reply = response.output_text

# âŒ åªå„²å­˜ assistant çš„å›æ‡‰
conversation_history.extend(response.output)

# âœ… åŒæ™‚å„²å­˜ user å’Œ assistant è¨Šæ¯
conversation_history.append({"role": "user", "content": user_message})
conversation_history.append({"role": "assistant", "content": assistant_reply})

# âœ… æ¯æ¬¡å‘¼å«éƒ½é‡æ–°æ³¨å…¥ PDF å…§å®¹
pdf_context = pdf_state.context_message()
if pdf_context:
    messages.append(pdf_context)

# âŒ ç›´æ¥æŠŠ response.output æ”¾å› input
conversation_history.extend(response.output)

# âœ… æ­£ç¢ºä½¿ç”¨ previous_response_id
last_response_id = getattr(response, "id", None)
if last_response_id:
    request_payload["previous_response_id"] = last_response_id

